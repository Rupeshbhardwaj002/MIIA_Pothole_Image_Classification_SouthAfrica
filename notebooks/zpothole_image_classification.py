# -*- coding: utf-8 -*-
"""ZPothole_Image_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NKwlPDBm9pdx1bKRh9-_4VOrw9vPk8vU
"""

#!pip install -q huggingface_hub

from huggingface_hub import hf_hub_download

zip_path = hf_hub_download(
    repo_id="rupesh002/Patholes_Dataset",
    repo_type="dataset",
    filename="pothole_dataset.zip"
)

import zipfile
import os


os.makedirs("pothole_data", exist_ok=True)


with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall("pothole_data")

import pandas as pd
train_csv = "pothole_data/pothole_dataset/train_ids_labels.csv"
test_csv = "pothole_data/pothole_dataset/test_ids_only.csv"

train_df = pd.read_csv(train_csv)
test_df = pd.read_csv(test_csv)

print(" Columns in training CSV:", list(train_df.columns), "\n")
print(train_df.head(), "\n")

# Fix test image paths
image_root = "pothole_data/pothole_dataset/all_data"

def find_image_path(img_id):
    for ext in [".jpg", ".jpeg", ".png", ".JPG", ".JPEG", ".PNG"]:
        path = os.path.join(image_root, img_id + ext)
        if os.path.exists(path):
            return path
    return None

test_df["Image_ID"] = test_df["Image_ID"].apply(find_image_path)
test_df = test_df.dropna().reset_index(drop=True)

print(f"âœ… Found {len(test_df)} test images")
print(test_df.head())

image_root = "pothole_data/pothole_dataset/all_data"
print(f"Found images in: {image_root}")

if "Image_ID" in train_df.columns:
    img_col = "Image_ID"
elif "image" in train_df.columns:
    img_col = "image"
elif "filename" in train_df.columns:
    img_col = "filename"
else:
    raise KeyError("Could nt find image column in train CSV. Check train_df.columns().")


def make_path(x):
    path = os.path.join(image_root, f"{x}.JPG")
    if not os.path.exists(path):
        path = os.path.join(image_root, f"{x}.jpg")
    return path

train_df[img_col] = train_df[img_col].apply(make_path)


train_df = train_df[train_df[img_col].apply(os.path.exists)].reset_index(drop=True)
print(f"All usable images: {len(train_df)}")
print(train_df.head())

print(train_df.head())
print(test_df.head())

from PIL import Image
import torch
from torch.utils.data import Dataset

class potholes(Dataset):
  def __init__ (self, dataframe, transform):
    self.df = dataframe
    self.transform = transform
  def __len__(self):
    return len(self.df)
  def __getitem__(self, index):
    image_path = self.df.iloc[index]["Image_ID"]
    label = self.df.iloc[index]["Label"]

    image = Image.open(image_path).convert("RGB")

    if self.transform:
      image = self.transform(image)

    return image, torch.tensor(label, dtype=torch.long)

from torchvision import transforms
from torch.utils.data import DataLoader

transform = transforms.Compose(
    [
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
    transforms.ToTensor(),
    ]
)

train_dataset = potholes(train_df,transform)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

import torch.nn as nn
import torchvision.models as models
model = models.resnet18(pretrained=True)
for param in model.parameters():
    param.requires_grad = True

model.fc = nn.Linear(model.fc.in_features,2 )

import torch.optim as optim

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

for epoch in range(15):  # increase epochs later
    model.train()
    running_loss = 0.0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    print(f"Epoch {epoch+1}, Loss: {running_loss:.4f}")

class PotholeTestDataset(Dataset):
    def __init__(self, dataframe, transform=None):
        self.df = dataframe
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        img_path = self.df.iloc[idx]["Image_ID"]
        image = Image.open(img_path).convert("RGB")
        if self.transform:
            image = self.transform(image)
        return image, self.df.iloc[idx]["Image_ID"]

test_dataset = PotholeTestDataset(test_df, transform=transform)
test_loader = DataLoader(test_dataset, batch_size=32)

model.eval()
predictions = []

with torch.no_grad():
    for images, ids in test_loader:
        images = images.to(device)
        outputs = model(images)
        probs = torch.softmax(outputs, dim=1)[:, 1]
        predictions.extend(zip(ids, probs.cpu().numpy()))

import pandas as pd

import pandas as pd


submission_df = pd.DataFrame(predictions, columns=["Image_Path", "Pothole_Probability"])

submission_df["Image_ID"] = submission_df["Image_Path"].apply(lambda x: x.split("/")[-1].split(".")[0])

submission_df = submission_df[["Image_ID", "Pothole_Probability"]]

submission_df["Pothole_Probability"] = submission_df["Pothole_Probability"].apply(lambda x: 1 if x > 0.5 else 0)

# save csv
submission_df.to_csv("final_submission.csv", index=False)

print(" Final submission file created: final_submission.csv")
submission_df.head()


from google.colab import files
files.download("final_submission.csv")

import joblib
joblib.dump(model, "model.pkl")
files.download('model.pkl')

!jupyter nbconvert --ClearMetadataPreprocessor.enabled=True --to notebook --output cleaned_notebook.ipynb Untitled0.ipynb